# -Contextualized-Sentiment-Classification-with-BERT-on-CFIMDB
 Implemented a BERT-based classifier for fine-tuned sentiment analysis on the CFIMDB dataset, leveraging pooled sentence embeddings and dropout-regularized projection layers. Developed data preprocessing and training pipelines to support both pretraining and downstream evaluation.
ğŸ“š Contextualized Sentiment Classification with BERT on CFIMDB
This project implements a BERT-based classifier for sentiment analysis on the CFIMDB dataset, adapted from CMUâ€™s 11-747: Neural Networks for NLP course. The system supports both pretraining and fine-tuning modes using Hugging Faceâ€™s transformers library.

ğŸ› ï¸ Key Components Implemented
BertSentClassifier (in classifier.py)

Encodes sentences using BERTâ€™s pooled output

Applies dropout regularization and linear transformation for classification

Supports both pretraining and task-specific fine-tuning

BertDataset & create_data

Loads and tokenizes raw text data

Maps sentences and labels to BERT-compatible input format (input IDs, attention masks, labels)

Training & Evaluation Pipelines

Fine-tuned BERT using custom run_exp.sh script

Logged training metrics and saved predictions for dev and test sets in both modes

ğŸ“ Directory Structure (Submission Format)
bash
Copy
Edit
CAMPUSID/
â”œâ”€â”€ classifier.py                      # Classifier logic with BERT-based architecture
â”œâ”€â”€ utils.py                           # Utility functions (no changes needed)
â”œâ”€â”€ README.md                          # This file
â”œâ”€â”€ cfimdb-dev-pretrain-output.txt    # Dev predictions (pretrain mode)
â”œâ”€â”€ cfimdb-test-pretrain-output.txt   # Test predictions (pretrain mode)
â”œâ”€â”€ cfimdb-train-pretrain-log.txt     # Training logs (pretrain)
â”œâ”€â”€ cfimdb-dev-finetune-output.txt    # Dev predictions (finetune mode)
â”œâ”€â”€ cfimdb-test-finetune-output.txt   # Test predictions (finetune mode)
â”œâ”€â”€ cfimdb-train-finetune-log.txt     # Training logs (finetune)
â”œâ”€â”€ run_exp.sh                         # Shell script for running experiments
â””â”€â”€ setup.sh                           # Environment and dependency setup
ğŸ”§ Setup Instructions
Run the following to set up dependencies and environment:

bash
Copy
Edit
bash setup.sh
ğŸš€ Running the Classifier
Replace CAMPUSID with your ID and run:

bash
Copy
Edit
mkdir -p CAMPUSID
python3 classifier.py --option finetune --epochs 3 --lr 2e-5 \
  --train data/sst-train.txt \
  --dev data/sst-dev.txt \
  --test data/sst-test.txt
Also supports --option pretrain for running in pretraining mode.

ğŸ“ˆ Output & Evaluation
All output files (dev/test predictions and training logs) are generated and stored in the respective text files, enabling reproducibility and evaluation across both modes.

ğŸ“œ Acknowledgements
This project builds upon the transformers library (Apache License 2.0) and CMUâ€™s CS11-747 NLP course codebase.
